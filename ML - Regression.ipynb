{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },{
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "T1 - LINEAR REGRESSION\n",
      "theta: [ 1.2943 -0.0585 -0.5894  0.4879  0.048 ]\n",
      "Mean Squared Error on the test set (Linear Regression): 0.68263\n",
      "Performance at task - kfold Cross Validation (k=10) - Estimated MSE for this model:  0.71681 +- 0.05555\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "T2 - RANDOM FOREST REGRESSOR - max_depth=30, n_estimators=400, random_state=80\n",
      "Mean Squared Error on the test set (Random Forest Regressor): 0.02161\n",
      "Performance at task - kfold Cross Validation (k=10) - Estimated MSE for this model:  0.01943 +- 0.00115\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "T1 vs T2 - LINEAR REGRESSION VS RANDOM FOREST REGRESSION\n",
      "T-test value: 13.841\n",
      "We can conclude that T2 performed statistically better than T1 with a 95% confidence interval since abs(13.841) > 1.96\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "The Dense Neural Network trained for 276 epochs - The early stopping function was triggered\n",
      "T3 - DENSE NEURAL NETWORK: 64 (tanh) - 32 (tanh) - 1, 276 epochs - batch size = 32 - adam optimizer\n",
      "Mean Squared Error on the test set (Neural Network): 0.01393\n",
      "Performance at task - kfold Cross Validation (k=10) - Estimated MSE for this model:  0.01310 +- 0.00128\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Assets written to: ram://e06cfd6e-a20b-48c9-809c-13134dcf427c/assets\n"
     ]
    }
   ],
   "source": [
    "# Importing the needed libraries\n",
    "\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import t\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import HeNormal\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "# Load data \n",
    "url = 'https://drive.switch.ch/index.php/s/TeDwnbYsBKRuJjv/download'\n",
    "response = requests.get(url)\n",
    "data = np.load(io.BytesIO(response.content))\n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "# T1 Linear Regression Model\n",
    "\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# creating arrays for the matrix X using the data x\n",
    "ones = np.ones(x.shape[0])     # creating an array of ones with the same length as x\n",
    "x1 = x[:,0]                    # selecting the first column of x: x1\n",
    "x2 = x[:,1]                    # selecting the second column of x: x2\n",
    "sin_x2 = np.sin(x2)            # computing the sine of each element of x2\n",
    "x1_x2 = x1*x2                  # computing the product of x1 and x2\n",
    "\n",
    "# creating the matrix X for linear regression by horizontally stacking the arrays\n",
    "X = np.hstack((ones.reshape(-1, 1), x1.reshape(-1, 1), x2.reshape(-1, 1), sin_x2.reshape(-1, 1), x1_x2.reshape(-1, 1)))\n",
    "\n",
    "# splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=80)\n",
    "\n",
    "# creating a linear regression model \n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# training process: fit the model to the training data \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predictions using the lr model\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# mse of the model on the test set\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "\n",
    "# coefficient of the linear regression model\n",
    "theta = lr.coef_\n",
    "\n",
    "# rounding the coefficients\n",
    "for index in range(len(theta)):\n",
    "    theta[index] = round(theta[index],4)\n",
    "\n",
    "# KFold approach to calculate the expected performance at task of the Linear Regression model and its confidence level\n",
    "\n",
    "# Create a KFold object\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=80)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "scores = cross_val_score(lr, X, y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "# The cross_val_score function by default calculates negative MSE for scoring\n",
    "\n",
    "mse_scores_lin = - scores\n",
    "\n",
    "# 95% confidence interval \n",
    "\n",
    "# confidence level\n",
    "confidence = 0.95\n",
    "\n",
    "# number of scores (MSEs - 10 in this case)\n",
    "n = len(mse_scores_lin)\n",
    "\n",
    "# standard deviation of the scores (MSEs)\n",
    "std_err = np.std(mse_scores_lin) / np.sqrt(n)\n",
    "\n",
    "# margin error\n",
    "margin_error95_lin = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "# print settings\n",
    "print(\"-\" * 130) \n",
    "print(\"T1 - LINEAR REGRESSION\")\n",
    "print(f\"theta: {theta}\")\n",
    "print(f\"Mean Squared Error on the test set (Linear Regression): {mse:.5f}\")\n",
    "print(\"Performance at task - kfold Cross Validation (k=10) - Estimated MSE for this model:  {:.5f} +- {:.5f}\".format(np.mean(mse_scores_lin),margin_error95_lin))\n",
    "\n",
    "# save the T1 - Linear Regression model to a pickle file\n",
    "with open('LinearRegressionModel.pickle', 'wb') as f:\n",
    "    pickle.dump(lr, f)\n",
    "\n",
    "print(\"-\" * 130)  \n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "# T2 Random Forest Regression Model\n",
    "\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# Split the data into 70% training set, 30% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=80)\n",
    "\n",
    "# initialization the param_grid for the GridSearchCV when fitting the training set\n",
    "param_grid = {'n_estimators': [100,300,400],'max_depth': [30,40,50],'random_state':[80]}\n",
    "\n",
    "# initialization of the model\n",
    "model_type = RandomForestRegressor()\n",
    "\n",
    "# initialization of the grid_search\n",
    "# specify scoring as 'neg_mean_squared_error'\n",
    "grid_search = GridSearchCV(model_type, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# fitting the grid_search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# this is the best model, based on the MSE of the validation set using a cv of 3\n",
    "rf = grid_search.best_estimator_\n",
    "\n",
    "# predict using best model\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# mse of the model on the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "# KFold approach to calculate the expected performance at task of the Random Forest Regression model and its confidence level\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=80)\n",
    "\n",
    "# performing k-fold cross-validation\n",
    "scores = cross_val_score(rf, x, y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "# the cross_val_score function by default calculates negative MSE for scoring\n",
    "\n",
    "mse_scores_rf = - scores\n",
    "\n",
    "# 95% confidence interval \n",
    "\n",
    "# confidence level\n",
    "confidence = 0.95\n",
    "\n",
    "# number of scores (MSEs - 10 in this case)\n",
    "n = len(mse_scores_rf)\n",
    "\n",
    "# standard deviation of the scores (MSEs)\n",
    "std_err = np.std(mse_scores_rf) / np.sqrt(n)\n",
    "\n",
    "# margin error\n",
    "margin_error95_rf = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "# print settings\n",
    "\n",
    "# getting all the parameters of the best model\n",
    "params = rf.get_params()\n",
    "# selecting the tuned ones (here random_state is fixed to 80 but was included for semplicity of the script)\n",
    "parameters_of_interest = {key: params[key] for key in ['max_depth', 'n_estimators', 'random_state']}\n",
    "# storing the tuned parameters in the params_str variable, for printing reasons\n",
    "params_str = ', '.join(f'{k}={v}' for k, v in parameters_of_interest.items())\n",
    "\n",
    "print(f\"T2 - RANDOM FOREST REGRESSOR - {params_str}\")\n",
    "print(f\"Mean Squared Error on the test set (Random Forest Regressor): {test_mse:.5f}\")\n",
    "print(\"Performance at task - kfold Cross Validation (k=10) - Estimated MSE for this model:  {:.5f} +- {:.5f}\".format(np.mean(mse_scores_rf),margin_error95_rf))\n",
    "\n",
    "# save the T2 - Random Forest Regressor model to a pickle file\n",
    "with open('RandomForestRegressorModel.pickle', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "print(\"-\" * 130) \n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "# T1 vs T2 - t test\n",
    "\n",
    "# T1\n",
    "\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# X matrix and splitting procedure\n",
    "ones = np.ones(x.shape[0]) ; x1 = x[:,0]  ; x2 = x[:,1]  ; sin_x2 = np.sin(x2)  ; x1_x2 = x1*x2                  \n",
    "X = np.hstack((ones.reshape(-1, 1), x1.reshape(-1, 1), x2.reshape(-1, 1), sin_x2.reshape(-1, 1), x1_x2.reshape(-1, 1)))\n",
    "\n",
    "# spliting the data into 70% training set, 30% test set (same random_state as above)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=80)\n",
    "\n",
    "# predictions using the lr model\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# mse of the lr model on the test set\n",
    "mse_lr = mean_squared_error(y_test,y_pred_lr)\n",
    "\n",
    "# residuals of the lr model\n",
    "res_lr = y_test - y_pred_lr\n",
    "\n",
    "# T2\n",
    "\n",
    "# spliting the data into 70% training set, 30% test set (same random_state as above)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=80)\n",
    "\n",
    "# predictions using the rf model\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# mse of the rf model on the test set\n",
    "mse_rf = mean_squared_error(y_test,y_pred_rf)\n",
    "\n",
    "# residuals of the rf model\n",
    "res_rf = y_test - y_pred_rf\n",
    "\n",
    "# -- T Test ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# MSEs\n",
    "mse1 = mse_lr ; mse2 = mse_rf \n",
    "\n",
    "# Ls\n",
    "l1 = len(y_test) ; l2 = len(y_test)\n",
    "\n",
    "# S1 and S2\n",
    "s1 = (1/(l1-1)) * np.sum ((((res_lr)**2) - mse_lr )**2)\n",
    "s2 = (1/(l2-1)) * np.sum ((((res_rf)**2) - mse_rf )**2)\n",
    "\n",
    "# T test value\n",
    "T = (mse1 - mse2) / (np.sqrt ((s1/l1)+(s2/l2)))\n",
    "\n",
    "# print setting\n",
    "print(\"T1 vs T2 - LINEAR REGRESSION VS RANDOM FOREST REGRESSION\")\n",
    "print(f\"T-test value: {T:.3f}\")\n",
    "\n",
    "if abs(T)>1.96:\n",
    "    print(f\"We can conclude that T2 performed statistically better than T1 with a 95% confidence interval since abs({T:.3f}) > 1.96\")\n",
    "else: \n",
    "    print(f\"We cannot conclude that T2 performed statistically better than T1 with a 95% confidence interval since abs({T:.3f}) < 1.96\")\n",
    "\n",
    "print(\"-\" * 130) \n",
    "\n",
    "#######################################################################################################################################\n",
    "\n",
    "# T3 Feed Feedforward Neural Network Model - Bonus Point\n",
    "\n",
    "# Setting random seeds for reproducibility of results\n",
    "keras.utils.set_random_seed(80)\n",
    "\n",
    "# x is a Numpy array with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array with the targets\n",
    "y = data.f.y\n",
    "\n",
    "\n",
    "#  Model ---------------------------------------------------------------------------------------------------------------\n",
    "# initializing the model\n",
    "nn = Sequential()  \n",
    "\n",
    "# Split the data into 70% training set, 15% validation set, and 15% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=80)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=80)\n",
    "\n",
    "# 1: dense layer with 64 neurons, using 'tanh' activation and input dimensions from X_train, he_init kernel_initializer\n",
    "# The He Normal initialization function initializes the weights of the layers using a normal distribution\n",
    "# with a mean of zero and a standard deviation calculated based on the number of input units to the layer\n",
    "\n",
    "he_init = HeNormal(seed=80)\n",
    "\n",
    "nn.add(Dense(64, input_dim=X_train.shape[1], activation='tanh', kernel_initializer=he_init))\n",
    "\n",
    "# 2: dense layer with 32 neurons and 'tanh' activation, he_init kernel_initializer\n",
    "nn.add(Dense(32, activation='tanh', kernel_initializer=he_init))\n",
    "\n",
    "# 3: final dense layer with 1 neuron (output layer) \n",
    "nn.add(Dense(1))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# compiling the model with the mean_squared_error loss function and the adam optimizer\n",
    "nn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# defining an early stopping function - to prevent overfitting\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# fitting the model with its 'history' (to retrieve the final number of epochs in the next line)\n",
    "history = nn.fit(X_train, y_train, epochs=400, batch_size=32, verbose=0, \n",
    "                  validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# retrieving the number of epochs that balances the bias-variance trade-off.\n",
    "num_epochs_trained = len(history.history['loss'])\n",
    "\n",
    "# how many epochs?? - printing this info in the console\n",
    "if num_epochs_trained == 400:\n",
    "    print('The Dense Neural Network trained for 400 epochs - The early stopping function was not triggered')\n",
    "else: \n",
    "    print(f'The Dense Neural Network trained for {num_epochs_trained} epochs - The early stopping function was triggered')\n",
    "\n",
    "# mse of the Feed Feedforward Neural Network model on the test set \n",
    "test_mse = nn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# print settings\n",
    "print(f\"T3 - DENSE NEURAL NETWORK: 64 (tanh) - 32 (tanh) - 1, {num_epochs_trained} epochs - batch size = 32 - adam optimizer\")\n",
    "print(f\"Mean Squared Error on the test set (Neural Network): {test_mse:.5f}\")\n",
    "\n",
    "# KFold approach to calculate the expected performance at task of the Feed Feedforward Neural Network model and its confidence level\n",
    "\n",
    "input_dim = x.shape[1]\n",
    "\n",
    "# defining a function that returns a compiled model - Keras models by default are not compatible with the scikit_learn kfold framework\n",
    "def create_model(input_dim):\n",
    "    nn = Sequential() ; he_init = HeNormal(seed=80)\n",
    "    nn.add(Dense(64, input_dim=input_dim, activation='tanh', kernel_initializer=he_init))\n",
    "    nn.add(Dense(32, activation='tanh', kernel_initializer=he_init))\n",
    "    nn.add(Dense(1)) ; nn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return nn\n",
    "\n",
    "# wrapping the Keras model in an estimator compatible with scikit_learn - epochs=num_epochs_trained!\n",
    "nn_sklearn = KerasRegressor(build_fn=lambda: create_model(input_dim), epochs=num_epochs_trained, batch_size=32, verbose=0)\n",
    "\n",
    "# using the Keras model with the cross_val_score tool in scikit_learn  \n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=80)\n",
    "scores = cross_val_score(nn_sklearn, x, y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "# the cross_val_score function by default calculates negative MSE for scoring\n",
    "\n",
    "mse_scores_nn = - scores\n",
    "\n",
    "# 95% confidence interval \n",
    "\n",
    "# confidence level\n",
    "confidence = 0.95\n",
    "\n",
    "# number of scores (MSEs - 10 in this case)\n",
    "n = len(mse_scores_nn)\n",
    "\n",
    "# standard deviation of the scores (MSEs)\n",
    "std_err = np.std(mse_scores_nn) / np.sqrt(n)\n",
    "\n",
    "# margin error\n",
    "margin_error95_nn = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "# print settings\n",
    "\n",
    "print(\"Performance at task - kfold Cross Validation (k=10) - Estimated MSE for this model:  {:.5f} +- {:.5f}\".format(np.mean(mse_scores_nn), margin_error95_nn))\n",
    "\n",
    "print(\"-\" * 130) \n",
    "\n",
    "# save the T3 - Neural Network model to a pickle file\n",
    "with open('NeuralNetworkModel.pickle', 'wb') as f:\n",
    "    pickle.dump(nn, f)\n",
    "\n",
    "#######################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "MSE on whole dataset: 0.012276736278965278\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import requests\n",
    "import io\n",
    "import joblib\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluates the mean squared error between the values in y_true and the values\n",
    "    in y_pred.\n",
    "    ### YOU CAN NOT EDIT THIS FUNCTION ###\n",
    "    :param y_true: Numpy array, the true target values from the test set;\n",
    "    :param y_pred: Numpy array, the values predicted by your model.\n",
    "    :return: float, the mean squared error between the two arrays.\n",
    "    \"\"\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Loads a Scikit-learn model saved with joblib.dump.\n",
    "    This is just an example, you can write your own function to load the model.\n",
    "    Some examples can be found in src/utils.py.\n",
    "    :param filename: string, path to the file storing the model.\n",
    "    :return: the model.\n",
    "    \"\"\"\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    return model\n",
    "\n",
    "#########################################################################################################\n",
    "######################################################################################################### \n",
    "\n",
    "# Load the data\n",
    "# This will be replaced with our private test data when grading the assignment\n",
    "\n",
    "# Load data from url\n",
    "url = 'https://drive.switch.ch/index.php/s/TeDwnbYsBKRuJjv/download'\n",
    "response = requests.get(url)\n",
    "data = np.load(io.BytesIO(response.content))\n",
    "\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# Data Preprocessing ---------> y in this case needed to be reshaped. If not... please remove this part\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "# Load the trained model\n",
    "nn_model = load_model(\"./NeuralNetworkModel.pickle\")\n",
    "\n",
    "# Predict on the given samples\n",
    "y_pred = nn_model.predict(x,verbose=0)\n",
    "print(\"-\" * 130)\n",
    "\n",
    "############################################################################\n",
    "# STOP EDITABLE SECTION: do not modify anything below this point.\n",
    "############################################################################\n",
    "\n",
    "# Evaluate the prediction using MSE\n",
    "mse = evaluate_predictions(y_pred, y)\n",
    "print(f'MSE on whole dataset: {mse}')\n",
    "\n",
    "# NOTE: NOW THIS CELL IS NOT WORKING SINCE YOU NEED TO CHANGE THE INPUT.\n",
    "# DO IT AND EVERYTHING RUNS SMOOTH\n",
    "\n",
    "############################################################################\n",
    "\n",
    "print(\"-\" * 130)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
